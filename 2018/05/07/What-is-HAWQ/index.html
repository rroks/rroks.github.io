<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--
          My life for si,
          though we can never go back.
          Fen Ch
    -->

    <!-- @
                                                       ▄              ▄
                                                      ▌▒█           ▄▀▒▌
            wow                                       ▌▒▒█        ▄▀▒▒▒▐
                                                     ▐▄▀▒▒▀▀▀▀▄▄▄▀▒▒▒▒▒▐
                                                   ▄▄▀▒░▒▒▒▒▒▒▒▒▒█▒▒▄█▒▐
                such code                         ▄▀▒▒▒░░░▒▒▒░░░▒▒▒▀██▀▒▌
                                                ▐▒▒▒▄▄▒▒▒▒░░░▒▒▒▒▒▒▒▀▄▒▒▌
                                                ▌░░▌█▀▒▒▒▒▒▄▀█▄▒▒▒▒▒▒▒█▒▐
                                               ▐░░░▒▒▒▒▒▒▒▒▌██▀▒▒░░░▒▒▒▀▄▌
                                               ▌░▒▄██▄▒▒▒▒▒▒▒▒▒░░░░░░▒▒▒▒▌
                          very html           ▀▒▀▐▄█▄█▌▄░▀▒▒░░░░░░░░░░▒▒▒▐
                                              ▐▒▒▐▀▐▀▒░▄▄▒▄▒▒▒▒▒▒░▒░▒░▒▒▒▒▌
                                              ▐▒▒▒▀▀▄▄▒▒▒▄▒▒▒▒▒▒▒▒░▒░▒░▒▒▐
                much css                       ▌▒▒▒▒▒▒▀▀▀▒▒▒▒▒▒░▒░▒░▒░▒▒▒▌
                                               ▐▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▒▄▒▒▐
                                                ▀▄▒▒▒▒▒▒▒▒▒▒▒░▒░▒░▒▄▒▒▒▒▌
                              so meta             ▀▄▒▒▒▒▒▒▒▒▒▒▄▄▄▀▒▒▒▒▄▀
                                                     ▀▄▄▄▄▄▄▀▀▀▒▒▒▒▒▄▄▀
                                                      ░▒▒▒▒▒▒▒▒▒▒▀▀
    -->

    <!--Description-->
    
        <meta name="description" content="Fen is here.">
    

    <!--Author-->
    
        <meta name="author" content="Fen Chy">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="What is HAWQ">
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="Fen is here.">
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="FenC">

    <!--Type page-->
    
        <meta property="og:type" content="article">
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary">
    

    <!-- Title -->
    
    <title>What is HAWQ - FenC</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Google Analytics -->
    


    <!-- favicon -->
    

</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">-.-. -.-.</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/rroks">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/0004.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>What is HAWQ</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2018-05-07
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <script src="/assets/js/APlayer.min.js"> </script><iframe width="100%" height="160" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Fsimgretina%2Fpeggy-suave-route-66&auto_play=true&hide_related=true&show_comments=true&show_user=true&show_reposts=false&color=ff5500&show_artwork=true"></iframe>
<p>最近需要学习一下 HAWQ，正好渣翻一下这个正在孵化的项目文档。</p>
<h1 id="What-is-HAWQ"><a href="#What-is-HAWQ" class="headerlink" title="What is HAWQ"></a>What is HAWQ</h1><p>HAWQ is a Hadoop native SQL query engine that combines the key technological advantages of MPP database with the scalability and convenience of Hadoop. HAWQ reads data from and writes data to HDFS natively.</p>
<pre><code>- HAWQ 是一个 Hadoop 原生 SQL 查询引擎，整合了 MPP 数据库的技术优点和 Hadoop 的方便性。HAWQ 从 HDFS 读取和写入数据。
</code></pre><p>HAWQ delivers industry-leading performance and linear scalability. It provides users the tools to confidently and successfully interact with petabyte range data sets. HAWQ provides users with a complete, standards compliant SQL interface. More specifically, HAWQ has the following features:</p>
<pre><code>- HAWQ 拥有业界领先的性能和现行扩展能力。它为使用者提供了可以放心、成功地和海量数据集交互的工具。HAWQ 为使用者提供了完全的、标准的 SQL 接口支持。更具体的，HAWQ 有以下特点：
</code></pre><p>On-premise or cloud deployment</p>
<pre><code>- 内部部署或云部署
</code></pre><p>Robust ANSI SQL compliance: SQL-92, SQL-99, SQL-2003, OLAP extension</p>
<pre><code>- 强健的 ANSI SQL 支持：SQL-92，SQL-99，SQL-2003，OLAP 扩展
</code></pre><p>Extremely high performance- many times faster than other Hadoop SQL engines</p>
<pre><code>- 极高的性能，数倍于其他Hadoop SQL 引擎
</code></pre><p>World-class parallel optimizer</p>
<pre><code>- 世界级的并行优化器
</code></pre><p>Full transaction capability and consistency guarantee: ACID</p>
<pre><code>- 完全事务支持和持久化保障：ACID
</code></pre><p>Dynamic data flow engine through high speed UDP based interconnect</p>
<pre><code>- 基于内联UDP网络的高速动态数据流引擎
</code></pre><p>Elastic execution engine based on on-demand virtual segments and data locality</p>
<pre><code>- 基于按需分配虚拟segment和数据局部性的全文执行引擎
</code></pre><p>Support multiple level partitioning and List/Range based partitioned tables.</p>
<pre><code>- 支持多级分区和基于 List/Range 的分区表
</code></pre><p>Multiple compression method support: snappy, gzip</p>
<pre><code>- 支持多种压缩方式：snappy，gzip
</code></pre><p>Multi-language user defined function support: Python, Perl, Java, C/C++, R</p>
<pre><code>- 多语言用户自定义函数支持：Python，Perl，Java，C/C++，R
</code></pre><p>Advanced machine learning and data mining functionalities through MADLib</p>
<pre><code>- MADLib提供的先进机器学习和数据挖掘方法
</code></pre><p>Dynamic node expansion: in seconds</p>
<pre><code>- 动态节点扩张：数秒内完成
</code></pre><p>Most advanced three level resource management: Integrate with YARN and hierarchical resource queues.</p>
<pre><code>- 最先进的三级资源管理：整合YARN和分层资源队列
</code></pre><p>Easy access of all HDFS data and external system data (for example, HBase)</p>
<pre><code>- 无障碍访问所有HDFS数据和外部系统数据（例如 HBase)
</code></pre><p>Hadoop Native: from storage (HDFS), resource management (YARN) to deployment (Ambari).</p>
<pre><code>- 原生Hadoop：从存储（HDFS），资源管理（YARN）到部署（Ambari）
</code></pre><p>Authentication &amp; granular authorization: Kerberos, SSL and role based access</p>
<pre><code>- 认证和简单授权：Kerberos，SSL和基于角色访问
</code></pre><p>Advanced C/C++ access library to HDFS and YARN: libhdfs3 and libYARN</p>
<pre><code>- 先进的 C/C++ HDFS和YARN访问库：libhdfs3 和 libYARN
</code></pre><p>Support for most third party tools: Tableau, SAS et al.</p>
<pre><code>- 支持大部分第三方工具：Tableau 和 SAS
</code></pre><p>Standard connectivity: JDBC/ODBC<br>    标准连接接口：JDBC/ODBC</p>
<p>HAWQ breaks complex queries into small tasks and distributes them to MPP query processing units for execution.</p>
<pre><code>- HAWQ 将复杂的查询分成多个小任务并分配给 MPP 查询处理单元执行。
</code></pre><p>HAWQ’s basic unit of parallelism is the segment instance. Multiple segment instances on commodity servers work together to form a single parallel query processing system. A query submitted to HAWQ is optimized, broken into smaller components, and dispatched to segments that work together to deliver a single result set. All relational operations - such as table scans, joins, aggregations, and sorts - simultaneously execute in parallel across the segments. Data from upstream components in the dynamic pipeline are transmitted to downstream components through the scalable User Datagram Protocol (UDP) interconnect.</p>
<pre><code>- HAWQ 的基本并行单元是 segment 实例。多个商用服务器上协同工作的 segment 实例组成了一个并行查询处理系统。被提交到 HAWQ 的语句会被优化，分成小的组件，打包发送至多个协同工作来交付同一个结果集的 segment 上。相关的操作，例如表的扫描、连接、聚合以及排列，同时在多个节点上并行执行。从上游动态管线组件来的数据通过可伸缩的 UDP 内联网络被发送到下游组件。
</code></pre><p>Based on Hadoop’s distributed storage, HAWQ has no single point of failure and supports fully-automatic online recovery. System states are continuously monitored, therefore if a segment fails, it is automatically removed from the cluster. During this process, the system continues serving customer queries, and the segments can be added back to the system when necessary.</p>
<pre><code>- 基于 Hadoop 的分布式存储，HAWQ 不存在单点失败，并且支持完全自动的在线恢复。系统状态处于持续监控，如果一个节点失败了，它将自动从集群中恢复。在这个过程中，HAWQ 系统将会继续处理用户查询，节点在需要时被添加回系统。
</code></pre><h1 id="HAWQ-Architecture"><a href="#HAWQ-Architecture" class="headerlink" title="HAWQ Architecture"></a>HAWQ Architecture</h1><p>In a typical HAWQ deployment, each slave node has one physical HAWQ segment, an HDFS DataNode and a NodeManager installed. Masters for HAWQ, HDFS and YARN are hosted on separate nodes.</p>
<pre><code>- 典型的 HAWQ 部署中，每个 slave 节点有一个物理 HAWQ segment，一个HDFS 数据节点，以及一个 节点管理器。HAWQ 的 master，HDFS 和 YARN 被放在不同的节点上。
</code></pre><p>The following diagram provides a high-level architectural view of a typical HAWQ deployment.</p>
<pre><code>- 下图展示了一个典型 HAWQ 部署的高层结构图。
</code></pre><p><img src="https://hdb.docs.pivotal.io/230/hawq/images/hawq_high_level_architecture.png" alt="HAWQ architectural"></p>
<p>HAWQ is tightly integrated with YARN, the Hadoop resource management framework, for query resource management. HAWQ caches containers from YARN in a resource pool and then manages those resources locally by leveraging HAWQ’s own finer-grained resource management for users and groups. To execute a query, HAWQ allocates a set of virtual segments according to the cost of a query, resource queue definitions, data locality and the current resource usage in the system. Then the query is dispatched to corresponding physical hosts, which can be a subset of nodes or the whole cluster. The HAWQ resource enforcer on each node monitors and controls the real time resources used by the query to avoid resource usage violations.</p>
<pre><code>- HAWQ 高度集成了 YARN，一个 Hadoop 资源管理框架，来负责管理查询资源。HAWQ 从 YARN 资源池中获取一个容器然后在本地通过 HAWQ 自己优化的资源管理器来为用户和小组管理资源。执行查询时，HAWQ 根据查询的需要的代价、资源队列定义、数据局部性以及当前系统的资源使用情况分配一组虚拟 segment。接下来查询将被分发至相应的物理节点（可以包括集群的子集或者整个集群）。每个节点上的 HAWQ 资源执行器监控和控制查询所用的资源来避免违规使用资源。
</code></pre><p>The following diagram provides another view of the software components that constitute HAWQ.</p>
<pre><code>- 下图展示了 HAWQ 结构的软件组成。
</code></pre><p><img src="https://hdb.docs.pivotal.io/230/hawq/images/hawq_high_level_architecture.png" alt="HAWQ software components"></p>
<h2 id="HAWQ-Master"><a href="#HAWQ-Master" class="headerlink" title="HAWQ Master"></a>HAWQ Master</h2><p>The HAWQ master is the entry point to the system. It is the database process that accepts client connections and processes the SQL commands issued. The HAWQ master parses queries, optimizes queries, dispatches queries to segments and coordinates the query execution.</p>
<pre><code>- HAWQ master 是系统的入口。它是接收客户连接和处理 SQL 命令的数据库。HAWQ master 处理查询，优化查询，分发查询到 segments 协同完成查询执行。
</code></pre><p>End-users interact with HAWQ through the master and can connect to the database using client programs such as psql or application programming interfaces (APIs) such as JDBC or ODBC.</p>
<pre><code>- 终端用户通过 master 和 HAWQ 交互，可以通过 psql 或者应用程序接口（例如 JDBC 或 ODBC）来连接数据库。
</code></pre><p>The master is where the global system catalog resides. The global system catalog is the set of system tables that contain metadata about the HAWQ system itself. The master does not contain any user data; data resides only on HDFS. The master authenticates client connections, processes incoming SQL commands, distributes workload among segments, coordinates the results returned by each segment, and presents the final results to the client program.</p>
<pre><code>- master 拥有全局系统表。全局系统表由一系列记录 HAWQ 系统元数据的系统表组成。master 不具有任何用户数据；数据只分布在 HDFS 上。master 验证客户连接，处理 SQL 命令，在 segment 间分配工作负载，
</code></pre><h2 id="HAWQ-Segment"><a href="#HAWQ-Segment" class="headerlink" title="HAWQ Segment"></a>HAWQ Segment</h2><p>In HAWQ, the segments are the units that process data simultaneously.</p>
<pre><code>- HAWQ segment 是并行处理数据的单元。
</code></pre><p>There is only one physical segment on each host. Each segment can start many Query Executors (QEs) for each query slice. This makes a single segment act like multiple virtual segments, which enables HAWQ to better utilize all available resources.</p>
<pre><code>- 每个主机上只有一个物理 segment。每个 segment 为每个查询片启动多个 查询执行器（QE)。这使得一个 segment 可以表现得像有多个虚拟 segment，以令 HAWQ 能更好地利用可用的资源。
</code></pre><p>A virtual segment behaves like a container for QEs. Each virtual segment has one QE for each slice of a query. The number of virtual segments used determines the degree of parallelism (DOP) of a query.</p>
<pre><code>- 虚拟 segment 是 QE 得容器。每个虚拟 segment 有一个 QE 处理一个查询片。虚拟 segment 得数量由查询的并行度决定。
</code></pre><p>A segment differs from a master because it:</p>
<pre><code>- segment 和 master 得不同主要体现在：
</code></pre><p>Is stateless.</p>
<pre><code>- 无状态
</code></pre><p>Does not store the metadata for each database and table.</p>
<pre><code>- 不存储数据库或表存储元数据
</code></pre><p>Does not store data on the local file system.</p>
<pre><code>- 不在本地文件系统存储数据
</code></pre><p>The master dispatches the SQL request to the segments along with the related metadata information to process. The metadata contains the HDFS url for the required table. The segment accesses the corresponding data using this URL.</p>
<pre><code>- master 分发 SQL 请求以及要处理的相关元数据到 segment 上。元数据包含需要用到的表的 HDFS URL 。segment 通过这个 URL 访问相应的数据。
</code></pre><h2 id="HAWQ-Interconnect"><a href="#HAWQ-Interconnect" class="headerlink" title="HAWQ Interconnect"></a>HAWQ Interconnect</h2><p>The interconnect is the networking layer of HAWQ. When a user connects to a database and issues a query, processes are created on each segment to handle the query. The interconnect refers to the inter-process communication between the segments, as well as the network infrastructure on which this communication relies. The interconnect uses standard Ethernet switching fabric.</p>
<pre><code>- 内联网络是 HAWQ 得网络层。当用户连接到数据库并进行查询，进程在每个 segment 上被创建来处理这个查询。内联网络代表 segment 得进程间通信也是这个通信所依赖的网络基础。内联网络使用标准的以太网交换结构。
</code></pre><p>By default, the interconnect uses UDP (User Datagram Protocol) to send messages over the network. The HAWQ software performs the additional packet verification beyond what is provided by UDP. This means the reliability is equivalent to Transmission Control Protocol (TCP), and the performance and scalability exceeds that of TCP. If the interconnect used TCP, HAWQ would have a scalability limit of 1000 segment instances. With UDP as the current default protocol for the interconnect, this limit is not applicable.</p>
<pre><code>- 内联网络默认使用 UDP 在网络上发送信息。HAWQ 提供了除了 UDP 之外的额外的包检验。这意味着它的可靠性等同于 TCP，性能和扩展性超过 TCP。如果内联网络使用 TCP，HAWQ 最多扩展至 1000个 segment 实例。内联网络默认使用 UDP 时没有这个限制。
</code></pre><h2 id="HAWQ-Resource-Manager"><a href="#HAWQ-Resource-Manager" class="headerlink" title="HAWQ Resource Manager"></a>HAWQ Resource Manager</h2><p>The HAWQ resource manager obtains resources from YARN and responds to resource requests. Resources are buffered by the HAWQ resource manager to support low latency queries. The HAWQ resource manager can also run in standalone mode. In these deployments, HAWQ manages resources by itself without YARN.</p>
<pre><code>- HAWQ 资源管理器从 YARN 获取资源并相应资源请求。HAWQ 资源管理器缓存资源来支持低延迟查询。HAWQ 资源管理器也能以独立模式运行。在这种部署下，HAWQ 资源管理器自己分配资源而不是通过 YARN。
</code></pre><p>See <a href="https://hdb.docs.pivotal.io/230/hawq/resourcemgmt/HAWQResourceManagement.html" target="_blank" rel="noopener">How HAWQ Manages Resources</a> for more details on HAWQ resource management.</p>
<h2 id="HAWQ-Catalog-Service"><a href="#HAWQ-Catalog-Service" class="headerlink" title="HAWQ Catalog Service"></a>HAWQ Catalog Service</h2><p>The HAWQ catalog service stores all metadata, such as UDF/UDT information, relation information, security information and data file locations.</p>
<pre><code>- HAWQ 目录服务保存了所有的元数据，例如 UDF/UDT 信息，关系信息，安全信息和数据文件位置。
</code></pre><h2 id="HAWQ-Fault-Tolerance-Service"><a href="#HAWQ-Fault-Tolerance-Service" class="headerlink" title="HAWQ Fault Tolerance Service"></a>HAWQ Fault Tolerance Service</h2><p>The HAWQ fault tolerance service (FTS) is responsible for detecting segment failures and accepting heartbeats from segments.</p>
<p>HAWQ 默认的错误容忍服务（FTS）负责检测 segment 的故障和接收 segment 的监测图。</p>
<p>See <a href="https://hdb.docs.pivotal.io/230/hawq/admin/FaultTolerance.html" target="_blank" rel="noopener">Understanding the Fault Tolerance Service</a> for more information on this service.</p>
<h2 id="HAWQ-Dispatcher"><a href="#HAWQ-Dispatcher" class="headerlink" title="HAWQ Dispatcher"></a>HAWQ Dispatcher</h2><p>The HAWQ dispatcher dispatches query plans to a selected subset of segments and coordinates the execution of the query. The dispatcher and the HAWQ resource manager are the main components responsible for the dynamic scheduling of queries and the resources required to execute them.</p>
<pre><code>- HAWQ 分发器分发查询计划给选定的 segment 子集并整合查询结果。分发器和 HAWQ 资源管理器是负责动态规划查询和执行查询的资源的主要组件。
</code></pre><h1 id="Table-Distribution-and-Storage"><a href="#Table-Distribution-and-Storage" class="headerlink" title="Table Distribution and Storage"></a>Table Distribution and Storage</h1><p>HAWQ stores all table data, except the system table, in HDFS. When a user creates a table, the metadata is stored on the master’s local file system and the table content is stored in HDFS.</p>
<pre><code>- HAWQ 在 HDFS 上存储除了系统表外的所有表数据。当用户创建一个表，元数据被存储在 master 的本地文件系统，表内容存储在 HDFS 上。
</code></pre><p>In order to simplify table data management, all the data of one relation are saved under one HDFS folder.</p>
<pre><code>- 为了简化表数据管理，所有属于同一关系的数据存放在相同 HDFS 文件夹下。
</code></pre><p>For all HAWQ table storage formats, AO (Append-Only) and Parquet, the data files are splittable, so that HAWQ can assign multiple virtual segments to consume one data file concurrently. This increases the degree of query parallelism.</p>
<pre><code>- 对于所有的 HAWQ 表存储格式，AO（只扩展）和 Parquet，数据文件是可分割的，因此 HAWQ 可以同时分配多个虚拟 segment 来使用同一个数据文件。这提升了查询并行度。
</code></pre><h2 id="Table-Distribution-Policy"><a href="#Table-Distribution-Policy" class="headerlink" title="Table Distribution Policy"></a>Table Distribution Policy</h2><p>The default table distribution policy in HAWQ is random.</p>
<pre><code>- HAWQ 默认的表分布策略是随机的。
</code></pre><p>Randomly distributed tables have some benefits over hash distributed tables. For example, after cluster expansion, HAWQ can use more resources automatically without redistributing the data. For huge tables, redistribution is very expensive, and data locality for randomly distributed tables is better after the underlying HDFS redistributes its data during rebalance or DataNode failures. This is quite common when the cluster is large.</p>
<pre><code>- 随机分布相遇对 Hash 分布有一些优点。 例如，在集群扩张后，HAWQ 可以自动地使用更多资源而不用重分布数据。对于大表，重分布的代价高昂，在 HDFS 通过再平衡或者数据节点故障进行数据重分布使数据局部性的表随机分布显得更优秀。在较大集群中，这样做比较常见。
</code></pre><p>On the other hand, for some queries, hash distributed tables are faster than randomly distributed tables. For example, hash distributed tables have some performance benefits for some TPC-H queries. You should choose the distribution policy that is best suited for your application’s scenario.</p>
<pre><code>- 另一方面，对于某一些查询，Hash 分布比随机分布更快。例如，对于 TPC-H 查询，Hash 分布有一些性能上的优势。用户应该选择最适合自己应用场景的分布策略。
</code></pre><p>See <a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/ddl/ddl-table.html" target="_blank" rel="noopener">Choosing the Table Distribution Policy</a> for more details.</p>
<h2 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h2><p>Data is distributed across HDFS DataNodes. Since remote read involves network I/O, a data locality algorithm improves the local read ratio. HAWQ considers three aspects when allocating data blocks to virtual segments:</p>
<pre><code>- 数据分布于 HDFS 数据节点上。远程读取数据需要涉及网络 I/O，因此数据局部性算法能够提升本地读取效率。HAWQ 依据以下三点分配数据块给虚拟 segment：
</code></pre><ol>
<li>Ratio of local read<ul>
<li>本地读取率</li>
</ul>
</li>
<li>Continuity of file read<ul>
<li>文件读取连续性</li>
</ul>
</li>
<li>Data balance among virtual segments<ul>
<li>虚拟 segment 间的数据平衡</li>
</ul>
</li>
</ol>
<h2 id="External-Data-Access"><a href="#External-Data-Access" class="headerlink" title="External Data Access"></a>External Data Access</h2><p>HAWQ can access data in external files using the HAWQ Extension Framework (PXF). PXF is an extensible framework that allows HAWQ to access data in external sources as readable or writable HAWQ tables. PXF has built-in connectors for accessing data inside HDFS files, Hive tables, and HBase tables. PXF also integrates with HCatalog to query Hive tables directly. See <a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/pxf/HawqExtensionFrameworkPXF.html" target="_blank" rel="noopener">Using PXF with Unmanaged Data</a> for more details.</p>
<pre><code>- HAWQ 可以通过使用 HAWQ 扩展框架（PXF）来访问外部文件。PXF 使 HAWQ 能够以可读或可写 HAWQ 表访问外部资源上的数据。PXF 具有内置的连接器用来访问 HDFS 文件，Hive 表以及 HBase 表。PXF 也整合了 HCatalog 来直接查询 Hive 表。
</code></pre><p>Users can create custom PXF connectors to access other parallel data stores or processing engines. Connectors are Java plug-ins that use the PXF API. For more information see <a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/pxf/PXFExternalTableandAPIReference.html" target="_blank" rel="noopener">PXF External Tables and API</a>.</p>
<pre><code>- 用户可以创建自定义 PXF 连接起来访问其他并行数据存储仓库或处理引擎。连接器是使用 PXF 接口的 Java 插件。
</code></pre><h1 id="Elastic-Query-Execution-Runtime"><a href="#Elastic-Query-Execution-Runtime" class="headerlink" title="Elastic Query Execution Runtime"></a>Elastic Query Execution Runtime</h1><p>HAWQ uses dynamically allocated virtual segments to provide resources for query execution.</p>
<pre><code>- HAWQ 使用动态分配的虚拟 segment 来为查询执行提供资源。
</code></pre><p>In HAWQ 1.x, the number of segments (compute resource carrier) used to run a query is fixed, no matter whether the underlying query is big query requiring many resources or a small query requiring little resources. This architecture is simple, however it uses resources inefficiently.</p>
<pre><code>- 在 HAWQ 1.x 版本中，用于执行查询的 segment 数量（计算资源载体）是固定的，无论执行的查询需要大量资源还是少量资源。这个结构很简单，但是不能有效地使用资源。
</code></pre><p>To address this issue, HAWQ now uses the elastic query execution runtime feature, which is based on virtual segments. HAWQ allocates virtual segments on demand based on the costs of queries. In other words, for big queries, HAWQ starts a large number of virtual segments, while for small queries HAWQ starts fewer virtual segments.</p>
<pre><code>- 为了解决这个问题，HAWQ 现在使用基于虚拟 segment 的运行时全文查询执行功能。HAWQ 根据查询所需的花费来决定分配的虚拟 segment。换句话说，对于大查询，HAWQ 启动较多的虚拟 segment，对于晓得查询 HAWQ 启动较少的虚拟 segment。
</code></pre><h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><p>In HAWQ, the number of invoked segments varies based on cost of query. In order to simplify table data management, all data of one relation are saved under one HDFS folder.</p>
<pre><code>- HAWQ 上依照查询花费启用不同数量的 segment。为了简化数据管理，所有属于同一个关系的数据保存在同一个 HDFS 文件夹下。
</code></pre><p>For all the HAWQ table storage formats, AO (Append-Only) and Parquet, the data files are splittable, so that HAWQ can assign multiple virtual segments to consume one data file concurrently to increase the parallelism of a query.</p>
<pre><code>- 对于所有的 HAWQ 表存储格式，AO 和 Parquet，数据文件是可分的。HAWQ可以分配多个虚拟节点来同时使用一个数据，以增加查询并行性。
</code></pre><h2 id="Physical-Segments-and-Virtual-Segments"><a href="#Physical-Segments-and-Virtual-Segments" class="headerlink" title="Physical Segments and Virtual Segments"></a>Physical Segments and Virtual Segments</h2><p>In HAWQ, only one physical segment needs to be installed on one host, in which multiple virtual segments can be started to run queries. HAWQ allocates multiple virtual segments distributed across different hosts on demand to run one query. Virtual segments are carriers (containers) for resources such as memory and CPU. Queries are executed by query executors in virtual segments.</p>
<pre><code>- 在 HAWQ 中，一个节点上只会安装一个物理 segment，但是可以启用多个虚拟 segment 来执行查询。HAWQ 在不同的主机上分配多个虚拟 segment 来执行一个查询。虚拟 segment 是资源（例如内存和 CPU ）的载体。查询由虚拟 segment 上的查询执行器执行。
</code></pre><h2 id="Virtual-Segment-Allocation-Policy"><a href="#Virtual-Segment-Allocation-Policy" class="headerlink" title="Virtual Segment Allocation Policy"></a>Virtual Segment Allocation Policy</h2><p>Different number of virtual segments are allocated based on virtual segment allocation policies. The following factors determine the number of virtual segments that are used for a query:</p>
<pre><code>- 根据虚拟 segment 分配策略会分配不同数量的虚拟 segment。以下因素决定用户查询的虚拟 segment 数量：
</code></pre><ol>
<li>Resources available at the query running time<ul>
<li>查询运行时可用的资源量</li>
</ul>
</li>
<li>The cost of the query<ul>
<li>查询的代价</li>
</ul>
</li>
<li>The distribution of the table; in other words, randomly distributed tables and hash distributed tables<ul>
<li>表的分布；换句话说，随机表分布和Hash表分布</li>
</ul>
</li>
<li>Whether the query involves UDFs and external tables<ul>
<li>当查询包含了 UDFs 和外部表</li>
</ul>
</li>
<li>Specific server configuration parameters, such as <code>default_hash_table_bucket_number</code> for hash table queries and <code>hawq_rm_nvseg_perquery_limit</code><ul>
<li>自定义服务器配置参数，例如 Hash 表查询 <code>default_hash_table_bucket_number</code>和<code>hawq_rm_nvseg_perquery_limit</code></li>
</ul>
</li>
</ol>
<h1 id="Resource-Management"><a href="#Resource-Management" class="headerlink" title="Resource Management"></a>Resource Management</h1><p>HAWQ provides several approaches to resource management and includes several user-configurable options, including integration with YARN’s resource management.</p>
<pre><code>- HAWQ 提供了多种资源管理的方法并打包了多个用户可配置选项，包括集成 YARN 资源管理器。
</code></pre><p>HAWQ has the ability to manage resources by using the following mechanisms:</p>
<pre><code>- HAWQ 可以通过以下功能管理资源：
</code></pre><ol>
<li>Global resource management. You can integrate HAWQ with the YARN resource manager to request or return resources as needed. If you do not integrate HAWQ with YARN, HAWQ exclusively consumes cluster resources and manages its own resources. If you integrate HAWQ with YARN, then HAWQ automatically fetches resources from YARN and manages those obtained resources through its internally defined resource queues. Resources are returned to YARN automatically when resources are not used anymore.<ul>
<li>全局资源管理。将 YARN 资源管理器整合进 HAWQ 来请求或返还资源。如果不将 YARN 与 HAWQ 整合，HAWQ 将只使用集群资源并只管理自己的资源。如果将 YARN 整合进 HAWQ，HAWQ 能自动地从 YARN 获取资源，并管理通过与定义的资源队列管理获取的资源。不再使用资源的时候，资源将被自动返还给 YARN。</li>
</ul>
</li>
<li>User defined hierarchical resource queues. HAWQ administrators or superusers design and define the resource queues used to organize the distribution of resources to queries.<ul>
<li>用户自定义多层级资源队列。HAWQ 管理员或者超级用户设计和定义用于组织查询资源分布的资源队列。</li>
</ul>
</li>
<li>Dynamic resource allocation at query runtime. HAWQ dynamically allocates resources based on resource queue definitions. HAWQ automatically distributes resources based on running (or queued) queries and resource queue capacities.<ul>
<li>运行时动态资源分配。HAWQ 根据资源队列的定义，动态地分配资源。HAWQ 根据正在运行的（或队列中的）查询以及可用的资源队列，自动地分配资源。</li>
</ul>
</li>
<li>Resource limitations on virtual segments and queries. You can configure HAWQ to enforce limits on CPU and memory usage both for virtual segments and the resource queues used by queries.<ul>
<li>虚拟 segment 和查询的资源限制。用户可以配置 HAWQ 为虚拟 segment 和被查询使用的资源队列添加 CPU 和 内存使用限制。</li>
</ul>
</li>
</ol>
<p>For more details on resource management in HAWQ and how it works, see <a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/resourcemgmt/HAWQResourceManagement.html" target="_blank" rel="noopener">Managing Resources</a>.</p>
<h1 id="HDFS-Catalog-Cache"><a href="#HDFS-Catalog-Cache" class="headerlink" title="HDFS Catalog Cache"></a>HDFS Catalog Cache</h1><p>HDFS catalog cache is a caching service used by HAWQ master to determine the distribution information of table data on HDFS.</p>
<pre><code>- HDFS 记录缓存用于缓存 HAWQ master 使用的服务，以确定 HDFS 上表数据的分布信息。
</code></pre><p>HDFS is slow at RPC handling, especially when the number of concurrent requests is high. In order to decide which segments handle which part of data, HAWQ needs data location information from HDFS NameNodes. HDFS catalog cache is used to cache the data location information and accelerate HDFS RPCs.</p>
<pre><code>- HDFS 处理 RPC 较慢，尤其是并发请求很高时。为了确定节点负责的数据部分，HAWQ 需要从 HDFS NameNode 获取数据定位信息。HDFS 目录缓存就是用来缓存数据定位信息和加速 HDFS RPC 的。
</code></pre><h1 id="High-Availability-Redundancy-and-Fault-Tolerance"><a href="#High-Availability-Redundancy-and-Fault-Tolerance" class="headerlink" title="High Availability, Redundancy and Fault Tolerance"></a>High Availability, Redundancy and Fault Tolerance</h1><p>HAWQ ensures high availability for its clusters through system redundancy. HAWQ deployments utilize platform hardware redundancy, such as RAID for the master catalog, JBOD for segments and network redundancy for its interconnect layer. On the software level, HAWQ provides redundancy via master mirroring and dual cluster maintenance. In addition, HAWQ supports high availability NameNode configuration within HDFS.</p>
<pre><code>- HAWQ 通过系统冗余保证了集群的高可用性。HAWQ 部署利用了平台硬件冗余，例如为 master的记录使用 RAID。JBOD 作为内联网络层，为 segment 和网络提供冗余。在软件层面，HAWQ 通过 master 镜像以及双重集群维护。此外，HAWQ 支持 HDFS 内配置的高可用的 NameNode。
</code></pre><p>To maintain cluster health, HAWQ uses a fault tolerance service based on heartbeats and on-demand probe protocols. It can identify newly added nodes dynamically and remove nodes from the cluster when it becomes unusable.</p>
<pre><code>- 保持集群的健康，HAWQ 使用了基于监测的错误容忍服务和按需探测原则。它可以动态地识别新添加的节点和从集群中移除不可用的节点。
</code></pre><h2 id="About-High-Availability"><a href="#About-High-Availability" class="headerlink" title="About High Availability"></a>About High Availability</h2><p>HAWQ employs several mechanisms for ensuring high availability. The foremost mechanisms are specific to HAWQ and include the following:</p>
<pre><code>- HAWQ 使用了多种结构来保证高可用性。重要的结构都是为 HAWQ 定制的，包括以下几点：
</code></pre><ol>
<li>Master mirroring. Clusters have a standby master in the event of failure of the primary master.<ul>
<li>master 镜像。集群有一个后备 master 以防备主 master 故障。</li>
</ul>
</li>
<li>Dual clusters. Administrators can create a secondary cluster and synchronizes its data with the primary cluster either through dual ETL or backup and restore mechanisms.<ul>
<li>双集群。管理员可以创建一个副集群然后通过双重 ETL 或者备份和保存结构来同步主集群的数据。</li>
</ul>
</li>
</ol>
<p>In addition to high availability managed on the HAWQ level, you can enable high availability in HDFS for HAWQ by implementing the high availability feature for NameNodes. See <a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/admin/HAWQFilespacesandHighAvailabilityEnabledHDFS.html" target="_blank" rel="noopener">HAWQ Filespaces and High Availability Enabled HDFS.</a></p>
<pre><code>- HAWQ 管理的额外高可用性，可以通过实现 NameNode 的高可用性特性来获得 HDFS 的高可用性。
</code></pre><h2 id="About-Segment-Fault-Tolerance"><a href="#About-Segment-Fault-Tolerance" class="headerlink" title="About Segment Fault Tolerance"></a>About Segment Fault Tolerance</h2><p>In HAWQ, the segments are stateless. This ensures faster recovery and better availability.</p>
<pre><code>- HAWQ segment 是无状态的。者保证了快速恢复和更高的可用性。
</code></pre><p>When a segment fails, the segment is removed from the resource pool. Queries are no longer dispatched to the segment. When the segment is operational again, the Fault Tolerance Service verifies its state and adds the segment back to the resource pool.</p>
<pre><code>- segment 故障时，节点会从资源池中移除。查询将不会被分发到这个 segment。当 segment 恢复可用时，错误容忍服务检验它的状态，然后将 segment 添加回资源池。
</code></pre><h2 id="About-Interconnect-Redundancy"><a href="#About-Interconnect-Redundancy" class="headerlink" title="About Interconnect Redundancy"></a>About Interconnect Redundancy</h2><p>The interconnect refers to the inter-process communication between the segments and the network infrastructure on which this communication relies. You can achieve a highly available interconnect by deploying dual Gigabit Ethernet switches on your network and deploying redundant Gigabit connections to the HAWQ host (master and segment) servers.</p>
<pre><code>- 内联网络指的是节点间进程通信和这个通信所依赖的网络。用户可以通过部署在网络上双重千兆以太网交换机和为 HAWQ 主机（master 和 segment）部署冗余千兆连接，以获得高可用性内联网络。
</code></pre><p>In order to use multiple NICs in HAWQ, NIC bounding is required.</p>
<pre><code>需要 NIC 绑定在 HAWQ 上使用多重 NIC。
</code></pre><h1 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h1><p><a href="http://hawq.incubator.apache.org/docs/userguide/2.2.0.0-incubating/overview/HAWQOverview.html" target="_blank" rel="noopener">HAWQ System Overview</a></p>
<p><a href="https://www.artstation.com/artwork/mynD8" target="_blank" rel="noopener"><img src="https://cdna.artstation.com/p/assets/images/images/010/673/970/large/yizheng-ke-2018-5-2.jpg?1525649991" alt="(｡・`ω´･)"></a></p>
<p>..-. . -.</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr>

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/rroks" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2018 Fen Chy<br></p>
                <p class="copyright text-muted">
                    <a target="_blank" href="http://www.gu6s.top/">闪电的6s</a>
                    <a target="_blank" href="http://gzh.autyan.net/">6s掌控雷电</a>
                    <a target="_blank" href="http://58.87.72.131:8080/w/hello">w</a>
                    <a target="_blank" href="http://druid.pub/">常威说他缺911</a>
                    <a target="_blank" href="http://www.yulaiz.com/">地王</a>
                    <a target="_blank" href="http://ikoala.net/">考拉</a>
                    <a target="_blank" href="http://baobaolei.cn/">大银蛇</a>
                    <a target="_blank" href="http://baobaolei.cn/">bydmm</a>
                    <a target="_blank" href="http://blog.autyan.com/">.net hentai</a>
                    <br>
                </p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>